# ğŸŸ£ Repo Scan Template

Use this template for **one file per scanned repo** under `Findings/Repo/`.

## File Template
```md
# ğŸŸ£ Repo <repo-name>

## ğŸ—ºï¸ Architecture Diagram
```mermaid
flowchart TB
  Dev[<repo-name> repo] --> CI[<CI/CD system>]
  CI --> Build[Build/Test]

  Build --> IaC[IaC / App]
  IaC --> Cloud[Cloud Resources]
  Cloud --> Deps[Service Dependencies]

  CI -. secrets scanning .-> Secrets[Secret scanning]
```

- **Description:** Security scan/triage summary for this repository.
- **Overall Score:** <severity emoji + label> <score>/10

## ğŸ§­ Overview
- **Repo path:** <absolute local path>
- **Repo URL (if applicable):** <url or N/A>
- **Scan scope:** SAST / dependency (SCA) / secrets / IaC / All
- **Languages/frameworks detected:** <e.g., Terraform, Go, Node.js (Express), Python (Django), .NET, Java (Spring), etc>
- **Evidence for detection:** <key files e.g., go.mod, package.json, pom.xml, requirements.txt, *.tf>
- **CI/CD:** <Azure Pipelines/GitHub Actions/etc>

## ğŸ›¡ï¸ Security Review
### Languages & Frameworks (extracted)
- <language/framework> â€” evidence: `<path>`

### ğŸ§¾ Summary
<short summary of material risks>

### âœ… Applicability
- **Status:** Yes / No / Donâ€™t know
- **Evidence:** <what was observed>

### âš ï¸ Assumptions
- <assumption that could change score/applicability> (mark as Confirmed/Unconfirmed)

### ğŸ¯ Exploitability
<how an attacker would realistically leverage issues>

### ğŸš© Risks
- <bullet list of notable risks/issues; link to related cloud/code findings if they exist>

### ğŸ” Key Evidence (deep dive)
Mark each deep-dive evidence item as positive/negative:
- âœ… = observed guardrail / good practice / risk reducer
- âŒ = observed weakness / insecure default / risk increaser

- âœ… <positive observation> â€” evidence: `<path:line>`
- âŒ <negative observation> â€” evidence: `<path:line>`

### Follow-up tasks for repo owners (optional)
- [ ] <what to verify> â€” evidence/source to check: `<path>`

### Cloud Environment Implications
Capture any **reusable** cloud context inferred from the repo (IaC + app config).
Promote reusable facts into `Knowledge/` as **Confirmed**/**Assumptions**.

- **Provider(s) referenced:** <Azure/AWS/GCP>
- **Cloud resources/services deployed or referenced:** <Key Vault, Storage, AKS, ACR, SQL, ...>
- **Network posture patterns:** <public endpoints, private endpoints, etc>
- **Identity patterns:** <managed identity/workload identity/roles>
- **Guardrails:** <policy-as-code, module standards, CI checks>

### Service Dependencies (from config / connection strings)
Extract downstream dependencies indicated by configuration, e.g.:
- DBs: Postgres/MySQL/MSSQL/Cosmos
- Queues/streams: Service Bus/Event Hub/Kafka
- Logs/telemetry: App Insights/Log Analytics/Datadog/Splunk
- APIs: internal/external base URLs

- **Datastores:** <...>
- **Messaging:** <...>
- **Logging/Monitoring:** <...>
- **External APIs:** <...>

### Containers / Kubernetes (inferred)
- If `skaffold.yaml`, Helm charts, or Kubernetes manifests are present: assume **Kubernetes** deploy.
- If `Dockerfile`(s) are present: assume a **container registry** is in use.
- If multiple Dockerfiles/Helm charts exist: identify base images (`FROM ...`) and note supply-chain risks.

- **Kubernetes tooling found:** <skaffold/helm/kustomize/manifests>
- **Container build artifacts:** <Dockerfile paths>
- **Base images:** <list of FROM images>

### âœ… Recommendations
- [ ] <recommendation> â€” â¬‡ï¸ <score>â¡ï¸<reduced-score> (est.)

### ğŸ“ Rationale
<why the score is what it is>

## ğŸ¤” Skeptic
> Purpose: review the **Security Review** above, then add what a security engineer would miss on a first pass.

### ğŸ› ï¸ Dev
- **Whatâ€™s missing/wrong vs Security Review:** <call out gaps, incorrect assumptions, or missing context>
- **Score recommendation:** â¡ï¸ Keep/â¬†ï¸ Up/â¬‡ï¸ Down â€” *explicitly state why vs the Security Review score*.
- **How it could be worse:** <realistic attacker path leveraging repo/app/IaC>
- **Countermeasure effectiveness:** <which recommendations measurably reduce risk; which are hygiene>
- **Assumptions to validate:** <which assumptions would change applicability/score>

### ğŸ—ï¸ Platform
- **Whatâ€™s missing/wrong vs Security Review:** <call out gaps, incorrect assumptions, or missing context>
- **Score recommendation:** â¡ï¸ Keep/â¬†ï¸ Up/â¬‡ï¸ Down â€” *explicitly state why vs the Security Review score*.
- **Operational constraints:** <pipelines, auth to cloud, network reachability, rollout sequencing>
- **Countermeasure effectiveness:** <guardrail coverage, drift prevention, enforceability>
- **Assumptions to validate:** <which assumptions would change applicability/score>

## ğŸ¤ Collaboration
- **Outcome:** <outcome>
- **Next step:** <next step>

## Compounding Findings
- **Compounds with:** <finding list or None identified>
  (use Markdown backlinks, e.g., `Findings/Cloud/Foo.md`)

## Meta Data
<!-- Meta Data must remain the final section in the file. -->
- ğŸ—“ï¸ **Last updated:** DD/MM/YYYY HH:MM
```
