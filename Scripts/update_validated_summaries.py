#!/usr/bin/env python3
from __future__ import annotations

import argparse
import re
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path


BOILERPLATE_SNIPPETS = [
    "This is a draft finding generated from a title-only input.",
    "Validate the affected resources/scope and confirm whether the exposure is internet-facing and/or impacts production workloads.",
]


@dataclass(frozen=True)
class ParsedFinding:
    path: Path
    title: str
    provider: str
    resource_type: str
    validation_status: str
    applicability_status: str
    confirmed_assumptions: list[str]
    compounds_with: str
    summary_lines: list[str]


def _now_stamp() -> str:
    # Match repo convention seen in findings: dd/mm/yyyy HH:MM
    return datetime.now().strftime("%d/%m/%Y %H:%M")


def _find_heading(lines: list[str], heading: str) -> int | None:
    for i, line in enumerate(lines):
        if line.strip() == heading:
            return i
    return None


def _slice_section_body(lines: list[str], heading_idx: int) -> tuple[int, int]:
    """
    Return (body_start_idx, body_end_exclusive_idx) for a markdown section.
    Section ends at next heading of same/higher level (##/###/#) or EOF.
    """
    start = heading_idx + 1
    end = len(lines)
    for j in range(start, len(lines)):
        if re.match(r"^#{1,3}\s+", lines[j]):
            end = j
            break
    return start, end


def _extract_first_match(lines: list[str], pattern: re.Pattern[str]) -> str:
    for line in lines:
        m = pattern.search(line)
        if m:
            return m.group(1).strip()
    return ""


def parse_finding(path: Path) -> ParsedFinding:
    lines = path.read_text(encoding="utf-8").splitlines()

    title = lines[0].lstrip("# ").strip() if lines else path.stem

    summary_heading = "### ðŸ§¾ Summary"
    summary_idx = _find_heading(lines, summary_heading)
    summary_lines: list[str] = []
    if summary_idx is not None:
        s0, s1 = _slice_section_body(lines, summary_idx)
        summary_lines = lines[s0:s1]

    applicability_heading = "### âœ… Applicability"
    applicability_idx = _find_heading(lines, applicability_heading)
    applicability_status = ""
    if applicability_idx is not None:
        a0, a1 = _slice_section_body(lines, applicability_idx)
        applicability_status = _extract_first_match(
            lines[a0:a1], re.compile(r"^\-\s+\*\*Status:\*\*\s*(.+?)\s*$")
        )

    assumptions_heading = "### âš ï¸ Assumptions"
    assumptions_idx = _find_heading(lines, assumptions_heading)
    confirmed_assumptions: list[str] = []
    if assumptions_idx is not None:
        u0, u1 = _slice_section_body(lines, assumptions_idx)
        for line in lines[u0:u1]:
            m = re.match(r"^\-\s+Confirmed\s+\(user\):\s*(.+?)\s*$", line.strip(), flags=re.IGNORECASE)
            if m:
                confirmed_assumptions.append(m.group(1).strip())

    compounds_with = _extract_first_match(lines, re.compile(r"^\-\s+\*\*Compounds with:\*\*\s*(.+?)\s*$"))
    provider = _extract_first_match(lines, re.compile(r"^\-\s+\*\*Provider:\*\*\s*(.+?)\s*$"))
    resource_type = _extract_first_match(lines, re.compile(r"^\-\s+\*\*Resource Type:\*\*\s*(.+?)\s*$"))
    validation_status = _extract_first_match(lines, re.compile(r"^\-\s+\*\*Validation Status:\*\*\s*(.+?)\s*$"))

    return ParsedFinding(
        path=path,
        title=title,
        provider=provider,
        resource_type=resource_type,
        validation_status=validation_status,
        applicability_status=applicability_status,
        confirmed_assumptions=confirmed_assumptions,
        compounds_with=compounds_with,
        summary_lines=summary_lines,
    )


def _contains_boilerplate(summary_lines: list[str]) -> bool:
    text = "\n".join(summary_lines).strip()
    if not text:
        return False
    return any(snippet in text for snippet in BOILERPLATE_SNIPPETS)

def _is_validated(validation_status: str) -> bool:
    return "validated" in validation_status.lower()


def _normalise_compounds_with(s: str) -> str:
    s = (s or "").strip()
    if not s:
        return ""
    if s.lower() in {"none", "none identified", "n/a"}:
        return ""
    return s


def _summary_text(summary_lines: list[str]) -> str:
    return "\n".join(summary_lines).strip()


def _summary_mentions_compounding(summary_lines: list[str]) -> bool:
    return "compounds with:" in _summary_text(summary_lines).lower()


def _looks_like_auto_summary(summary_lines: list[str]) -> bool:
    """
    Heuristic: only modify non-boilerplate summaries when they still look like
    they were generated by this tool, to avoid overwriting a human-written summary.
    """
    text = _summary_text(summary_lines).lower()
    return ("validated context (user):" in text) or text.startswith("this finding is not applicable")

def _should_refresh_summary(p: ParsedFinding) -> bool:
    """
    Refresh summaries when there's new information available, but only for findings that are:
      - validated, OR
      - have applicability set to Yes/No, OR
      - contain at least one confirmed (user) assumption.

    This avoids rewriting pure title-only drafts where no additional context exists yet.
    """
    if not _contains_boilerplate(p.summary_lines):
        # Still allow a refresh to incorporate compounding context, but only when:
        #  - compounding is present, and
        #  - the existing summary looks auto-generated, and
        #  - the summary doesn't already mention compounding.
        compounds = _normalise_compounds_with(p.compounds_with)
        if compounds and _looks_like_auto_summary(p.summary_lines) and not _summary_mentions_compounding(p.summary_lines):
            return True
        return False

    if _is_validated(p.validation_status):
        return True

    status = _normalise_status(p.applicability_status)
    if status in {"yes", "no"}:
        return True

    if p.confirmed_assumptions:
        return True

    return False


def _normalise_status(s: str) -> str:
    s = (s or "").strip().lower()
    if s in {"yes", "y"}:
        return "yes"
    if s in {"no", "n", "not applicable", "n/a"}:
        return "no"
    if "donâ€™t know" in s or "don't know" in s:
        return "dont_know"
    return s


def _pick_context_snippets(confirmed: list[str]) -> list[str]:
    # Prefer concise, high-signal context; cap to avoid overly-long summaries.
    prefer_needles = [
        "production",
        "internet-facing",
        "application gateway",
        "no private endpoints",
        "public network access",
        "all networks",
        "no firewall",
        "store production secrets",
        "store production secrets/keys",
    ]
    picks: list[str] = []
    remaining = list(confirmed)
    for needle in prefer_needles:
        for c in list(remaining):
            if needle in c.lower():
                picks.append(c)
                remaining.remove(c)
                break
        if len(picks) >= 3:
            break
    while remaining and len(picks) < 3:
        picks.append(remaining.pop(0))
    return picks


def _business_impact_sentence(title: str, resource_type: str, confirmed: list[str]) -> str:
    t = (title or "").lower()
    rt = (resource_type or "").lower()
    ctx = " ".join(c.lower() for c in confirmed)

    is_prod = "production" in ctx
    prod_prefix = "In production, " if is_prod else ""

    if "key vault" in t or "key vault" in rt or "keyvault" in t or "keyvault" in rt:
        if "store production secrets" in ctx or "secrets/keys" in ctx or "keys" in ctx:
            return (
                f"{prod_prefix}publicly reachable Key Vaults that store application secrets/keys increase the risk of credential theft, "
                "leading to downstream service compromise."
            )
        return f"{prod_prefix}publicly reachable Key Vaults increase the risk of secret/key access outside intended network boundaries."

    if "storage" in t or "storage account" in rt or "blob" in t:
        return f"{prod_prefix}Storage accounts reachable via public endpoints increase the risk of data exposure or destructive access."

    if "nsg" in t or "network security group" in t or "networking" in rt:
        return f"{prod_prefix}overly broad network rules increase the likelihood of unauthorized access to workloads and lateral movement."

    if "sql" in t or "database" in rt:
        return f"{prod_prefix}database network misconfiguration increases the risk of unauthorized access and data exfiltration."

    if "kubernetes" in rt or "aks" in t:
        return f"{prod_prefix}AKS access-control gaps increase the risk of cluster compromise and workload takeover."

    if "virtual machine" in rt or "vm" in t:
        return f"{prod_prefix}VM configuration gaps increase the likelihood of initial access and privilege escalation."

    return f"{prod_prefix}this configuration increases the risk of unauthorized access and business-impacting compromise."


def generate_summary(p: ParsedFinding) -> str:
    status = _normalise_status(p.applicability_status)
    compounds = _normalise_compounds_with(p.compounds_with)

    if status == "no":
        # Prefer a succinct "not applicable" summary.
        reason = ""
        for c in p.confirmed_assumptions:
            if "rbac" in c.lower() or "access policies" in c.lower():
                reason = c.rstrip(".")
                break
        if reason:
            base = f"This finding is not applicable: {reason}."
        else:
            base = "This finding is not applicable based on current confirmed configuration."
        return f"{base}\n\nCompounds with: {compounds}." if compounds else base

    impact = _business_impact_sentence(p.title, p.resource_type, p.confirmed_assumptions)

    ctx_picks = _pick_context_snippets(p.confirmed_assumptions)
    if ctx_picks:
        # Second sentence: brief context, without over-claiming specific resource IDs.
        context = "; ".join(s.rstrip(".") for s in ctx_picks)
        base = f"{impact}\n\nValidated context (user): {context}."
        return f"{base}\n\nCompounds with: {compounds}." if compounds else base

    return f"{impact}\n\nCompounds with: {compounds}." if compounds else impact


def _replace_summary(lines: list[str], new_summary: str) -> list[str]:
    summary_heading = "### ðŸ§¾ Summary"
    idx = _find_heading(lines, summary_heading)
    if idx is None:
        return lines
    s0, s1 = _slice_section_body(lines, idx)

    new_block = new_summary.splitlines()
    # Ensure at least one blank line between summary and next heading
    if new_block and new_block[-1].strip() != "":
        new_block.append("")

    return lines[:s0] + new_block + lines[s1:]


def _update_last_updated(lines: list[str]) -> list[str]:
    stamp = _now_stamp()
    out: list[str] = []
    updated = False
    for line in lines:
        if re.match(r"^\-\s*ðŸ—“ï¸\s*\*\*Last updated:\*\*", line):
            out.append(f"- ðŸ—“ï¸ **Last updated:** {stamp}")
            updated = True
        else:
            out.append(line)
    return out if updated else lines


def iter_findings(root: Path) -> list[Path]:
    if root.is_file() and root.suffix.lower() == ".md":
        return [root]
    if not root.exists():
        return []
    return sorted([p for p in root.rglob("*.md") if p.is_file()])


def main() -> int:
    ap = argparse.ArgumentParser(description="Update finding summaries to remove title-only boilerplate and incorporate compounding context.")
    ap.add_argument("--path", default="Output/Findings/Cloud", help="Finding file or folder to scan")
    ap.add_argument("--in-place", action="store_true", help="Write changes to disk (default: dry-run)")
    args = ap.parse_args()

    root = Path(args.path)
    paths = iter_findings(root)
    if not paths:
        print(f"No markdown findings found under: {root}")
        return 1

    changed = 0
    skipped = 0
    for path in paths:
        parsed = parse_finding(path)
        if not _should_refresh_summary(parsed):
            skipped += 1
            continue

        lines = path.read_text(encoding="utf-8").splitlines()
        new_summary = generate_summary(parsed)
        updated_lines = _replace_summary(lines, new_summary)
        updated_lines = _update_last_updated(updated_lines)

        if updated_lines == lines:
            skipped += 1
            continue

        changed += 1
        if args.in_place:
            path.write_text("\n".join(updated_lines) + "\n", encoding="utf-8")
            print(f"UPDATED: {path}")
        else:
            print(f"WOULD UPDATE: {path}")

    print(f"Scanned: {len(paths)} | changed: {changed} | skipped: {skipped} | mode: {'in-place' if args.in_place else 'dry-run'}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
