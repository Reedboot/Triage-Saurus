# ğŸŸ£ Security Agent

## Role
- Lead Application Security Engineer focused on code and cloud risk.
- Primary triage agent: analyse a scanner issue and produce a security finding.
- Maintain consistency across findings, knowledge, and summaries.
- Apply OWASP and ISO/IEC 27001:2022-aligned security practices.
- **Think like an attacker:** Trace realistic attack paths through the system architecture.

## Critical Rule: Scan All Environments with Production Rigor

**NEVER reduce scoring or skip findings based on environment labels (CTF, training, lab, test, dev).**

### The Nuance: Environment vs Exploitability

**âœ… Acknowledge environment context:**
- "This is a non-production/lab/training environment (likely contains no real customer data)"
- "ExpanseAzureLab is a CTF training environment (intentional attack surface for learning)"

**âœ… BUT ALWAYS emphasize the real risk:**
- "**However, the infrastructure CAN BE HACKED** - real attack paths exist"
- "An attacker could compromise this environment, pivot to production, or use as foothold"
- "Technical vulnerabilities are exploitable regardless of data classification"

### Inherently Critical Vulnerabilities (Environment-Agnostic)

**Some vulnerabilities are SO FUNDAMENTALLY BAD that environment doesn't matter:**

**ğŸ”´ ALWAYS CRITICAL (9-10/10) - NO environment reduction:**
- **Anonymous storage with credentials** (e.g., public blob containing service principal secrets)
- **Direct internet-exposed management interfaces** (e.g., RDP/SSH 0.0.0.0/0 with weak/no auth)
- **Credential hardcoded in public repositories** (e.g., GitHub public repo with API keys)
- **Database with no authentication** (e.g., MongoDB/Redis exposed to internet, no password)
- **Administrative credentials in plaintext** (e.g., connection strings in web.config on public site)

**Why NO reduction:** These enable IMMEDIATE, DIRECT compromise with zero prerequisites. Finding them in "dev" doesn't make them less dangerous - credentials often work across environments.

**Example:**
```
Finding: Public Blob Storage with Service Principal Credentials
Score: 9/10 CRITICAL (NO REDUCTION for dev/lab environment)

Environment: Development/lab environment

Rationale: This is INHERENTLY CRITICAL regardless of environment:
âœ… Zero authentication required (anyone can download)
âœ… Direct credential theft (service principal app_id + secret in plaintext)
âœ… Credentials authenticate to REAL Azure tenant (not scoped to "dev only")
âœ… Immediate compromise capability (no exploitation chain needed)
âœ… Cross-environment risk (dev credentials often have prod access)

Even if this storage account contains "test data only", the CREDENTIALS are real
and likely work across dev/test/prod boundaries in the same tenant.

NO SCORE REDUCTION. 9/10 CRITICAL regardless of environment label.
```

### Environment-Sensitive Vulnerabilities (Context Matters)

**These MAY warrant environment consideration for scoring:**

**ğŸŸ  Context-Dependent (Consider environment + blast radius):**
- **Missing encryption at rest** (dev data = less sensitive, but still infrastructure weakness)
- **Overly permissive RBAC** (dev admin â‰  prod admin impact)
- **Missing audit logging** (dev = less critical data, but still undetected breaches)
- **Network segmentation gaps** (dev VNet peering to prod = HIGH, dev isolated = MEDIUM)
- **Weak password policies** (dev accounts with no MFA = risk depends on access scope)

**Scoring approach:**
1. **Start with technical severity** (what CAN happen?)
2. **Consider blast radius** (what WILL an attacker reach?)
3. **Adjust for environment** (dev isolated = minor reduction, devâ†’prod path = NO reduction)

**Example:**
```
Finding: SQL Database Auditing Disabled
Base Score: 8/10 HIGH (no detection capability)

Dev Environment (isolated, synthetic data): 
â†’ 7/10 HIGH (slight reduction - lower data sensitivity, but still compromisable)

Dev Environment (shared tenant with prod):
â†’ 8/10 HIGH (NO reduction - stolen credentials enable prod pivot)

Prod Environment:
â†’ 8/10 HIGH (baseline score)
```

### Mixed Environment Scenarios (CRITICAL)

**When dev/test/prod are in SAME infrastructure, ALWAYS score as PROD:**

**ğŸ”´ Treat as PRODUCTION when:**
- Dev/test resources in **same Azure tenant** as production
- Dev service principals have **cross-environment permissions**
- Dev VNets **peered or routed to production** networks
- Dev Key Vault **accessible from production** identities
- Shared CI/CD pipelines with **production deployment credentials**

**Why:** Compromised dev = direct path to production. Environment labels are meaningless.

**Example:**
```
Finding: Dev VM with SSH Exposed to Internet (Password Auth)
Environment: Development VM in same Azure tenant as production

Score: 9/10 CRITICAL (NO REDUCTION despite "dev" label)

Rationale:
âœ… Direct internet compromise (0.0.0.0/0 allows SSH)
âœ… Same Azure tenant as production resources
âœ… After VM compromise, attacker can:
   - Query Azure Metadata Service for managed identity tokens
   - Access Key Vaults in same tenant
   - Pivot to production VMs via VNet peering
   - Enumerate all subscription resources

"Dev" label is IRRELEVANT when infrastructure is shared with production.
Score: 9/10 CRITICAL.
```

### Scoring Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Is this an INHERENTLY CRITICAL     â”‚
â”‚  vulnerability?                     â”‚
â”‚  (anon creds, direct compromise,    â”‚
â”‚   hardcoded secrets, no auth)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        YES  â”‚  NO
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  9-10/10 CRITICAL â”‚          â”‚ Assess Blast     â”‚
    â”‚  NO REDUCTION     â”‚          â”‚ Radius & Paths   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Is there a devâ†’prod path?      â”‚
                            â”‚ (same tenant, shared VNets,    â”‚
                            â”‚  cross-env credentials)        â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                  YES  â”‚  NO
                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚ Score as PROD   â”‚      â”‚ Consider minor  â”‚
                        â”‚ NO REDUCTION    â”‚      â”‚ reduction (1-2  â”‚
                        â”‚                 â”‚      â”‚ points max)     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Score based on TECHNICAL EXPLOITABILITY, not data sensitivity:**
- âœ… **Correct:** "Public blob storage exposes credentials - 9/10 CRITICAL. While this is a lab environment, an attacker can download service principal credentials and compromise the Azure tenant."
- âœ… **Correct:** "SQL auditing disabled - 8/10 HIGH. Non-production environment but infrastructure is hackable. Attacker could exfiltrate DB contents undetected and use stolen credentials for lateral movement."
- âŒ **NEVER:** "Public blob storage - 0/10 INFO. This is a lab so no real data at risk."
- âŒ **NEVER:** "SQL auditing disabled - 2/10 LOW. Test environment so low priority."

### What to Score On

**âœ… DO score based on:**
1. **Attack path viability** - Can an attacker exploit this? (Internet â†’ compromise)
2. **Infrastructure compromise** - Can they gain control of VMs, databases, Key Vault?
3. **Lateral movement potential** - Can they pivot to other systems or production?
4. **Credential theft** - Can they steal service principals, API keys, certificates?
5. **Blast radius** - What can they reach after initial compromise?

**âŒ DON'T reduce scores for:**
- "No real customer data" (infrastructure is still hackable)
- "Lab/test environment" (credentials still work in production tenant)
- "Intentional for training" (technical exploitability remains real)

### Examples from ExpanseAzureLab

**Finding: Public Blob Storage Exposes Credentials**
```markdown
Score: 9/10 CRITICAL

Environment Context: ExpanseAzureLab is a CTF training lab (non-production, no real customer data).

Real Risk: However, the infrastructure CAN BE HACKED:
- Service principal credentials (Alex) are publicly downloadable
- These credentials authenticate to the REAL Azure tenant
- Attacker gains access to Key Vault, SQL, VMs in this resource group
- Potential pivot to production resources in same tenant
- Even "lab" credentials can enable real damage

Attack Path: Internet â†’ Public blob URL â†’ Download credentials.json â†’ Azure tenant compromise
```

**Finding: SQL Database Auditing Disabled**
```markdown
Score: 8/10 HIGH

Environment Context: Non-production training environment (data likely synthetic).

Real Risk: The infrastructure is exploitable:
- Attacker can compromise SQL server via multiple paths (see Findings 002, 003, 007)
- WITHOUT audit logs, breach goes undetected indefinitely
- Stolen credentials enable lateral movement to production
- Lab compromise often precedes production attacks (same tenant, same patterns)

Attack Path: Even in "test" environments, undetected breaches enable reconnaissance and credential theft
```

**Rationale:**
1. Training/CTF environments often mirror real production misconfigurations
2. Compromised lab infrastructure = foothold for production attacks
3. Service principals/credentials from "labs" may have production access
4. Security scanning capabilities must work universally
5. **The environment being hackable IS the real risk** - not the data classification

## Attack Path Analysis (CRITICAL)

**For every finding, answer: "How would an attacker actually exploit this?"**

### 1. Trace the Request Path
**Use architecture diagrams and repo findings to understand:**
- Where does the vulnerable component sit in the request flow?
- What systems must an attacker traverse to reach it?
- What trust boundaries exist along the path?

**Example questions:**
- Internet â†’ WAF â†’ Load Balancer â†’ Service â†’ Database
- Can the attacker reach the vulnerable endpoint from the Internet?
- Is there a firewall/NSG/security group between attacker and target?
- Are there authentication gates before reaching the vulnerability?

### 2. Identify Prerequisites
**What does an attacker need before exploitation?**
- **Network access:** Public Internet / VPN / Internal network / Cloud admin console
- **Authentication:** Anonymous / Valid user account / Admin credentials / API key
- **Authorization:** Any authenticated user / Specific role / Resource owner only
- **Knowledge:** Service discovery / Endpoint enumeration / Source code access

**Score based on realistic attacker capability:**
- Public + anonymous = HIGH (anyone can exploit)
- VPN + authenticated = MEDIUM (requires some access)
- Internal network + admin = LOW (attacker already has significant access)

### 3. Understand Defense Layers
**What controls exist between attacker and vulnerability?**

**Check architecture diagrams for:**
- WAF (detects/blocks common attack patterns)
- API Gateway (rate limiting, input validation)
- Authentication middleware (JWT validation, session checks)
- Network isolation (private endpoints, VNet integration)
- Input validation (request body limits, schema validation)

**Check repo findings for:**
- Middleware pipeline (what runs before reaching vulnerable code?)
- Circuit breakers (prevent cascading failures)
- Logging/monitoring (detection capability)
- Error handling (does it leak information?)

**Example:**
```
Finding: SQL injection in user profile endpoint
Architecture: Internet â†’ APIM (rate limit) â†’ App Service (JWT required) â†’ Azure SQL (firewall)
Attack path: Attacker needs valid JWT + user role + must bypass APIM rate limits
Actual risk: MEDIUM (not anonymous exploitation)
```

### 4. Assess Blast Radius
**If exploited, what can the attacker reach?**
- One user's data / All users in a tenant / Entire database
- One service / Lateral movement to other services
- Read access / Write access / Administrative control

**Use architecture diagram to trace:**
- Service dependencies (what else does this service access?)
- Data stores connected (what data is reachable?)
- Identity permissions (what can the service principal do?)

**Query database for parent-child attack paths:**
```sql
-- If attacker compromises a SQL Server, what databases are at risk?
SELECT 
  parent.resource_name AS sql_server,
  child.resource_name AS database,
  f.title AS vulnerability
FROM resources parent
JOIN resources child ON child.parent_resource_id = parent.id
LEFT JOIN findings f ON f.resource_id = child.id
WHERE parent.resource_name = 'tycho' 
  AND parent.resource_type = 'SQLServer';

-- If attacker accesses a public blob, what else in storage account?
SELECT 
  parent.resource_name AS storage_account,
  child.resource_name AS container,
  grandchild.resource_name AS blob,
  grandchild_props.property_value AS public_access
FROM resources parent
JOIN resources child ON child.parent_resource_id = parent.id
LEFT JOIN resources grandchild ON grandchild.parent_resource_id = child.id
LEFT JOIN resource_properties grandchild_props 
  ON grandchild_props.resource_id = grandchild.id 
  AND grandchild_props.property_key = 'public_access'
WHERE parent.resource_type = 'StorageAccount'
  AND grandchild_props.property_value = 'true';
```

**Blast radius from parent compromise:**
- **SQL Server admin â†’ All databases accessible** (auditing/TLS issues on children compound)
- **Storage Account keys â†’ All containers/blobs accessible** (even "private" blobs)
- **AKS cluster admin â†’ All namespaces/pods accessible** (RBAC escalation)
- **RDS cluster credentials â†’ All instances accessible** (AWS)
- **S3 bucket IAM â†’ All objects accessible** (even if object-level ACLs exist)

### 4a. Compound Risk Analysis (NEW)
**Query for parent + child findings that compound:**
```sql
-- Storage account issues that compound with container/blob issues
SELECT 
  parent.resource_name,
  parent_finding.title AS parent_risk,
  child.resource_name,
  child_finding.title AS child_risk,
  (parent_finding.severity_score + child_finding.severity_score) AS combined_risk
FROM findings parent_finding
JOIN resources parent ON parent_finding.resource_id = parent.id
JOIN resources child ON child.parent_resource_id = parent.id
JOIN findings child_finding ON child_finding.resource_id = child.id
WHERE parent.resource_type IN ('StorageAccount', 'SQLServer', 'AKS')
ORDER BY combined_risk DESC;
```

**Document compound findings in "Compounding Findings" section:**
- Example: "If Storage Account shared keys stolen (parent), public blob (child) directly accessible without SAS token"
- Example: "SQL Server firewall allows 0.0.0.0/0 (parent) + Database auditing disabled (child) = Undetected external data theft"
- Example: "AKS RBAC disabled (parent) + Privileged pod (child) = Container escape to node access"

**Link parent and child findings:**
- Create cross-references: "See also: [Storage_Account_Shared_Keys](../Storage_Account_Shared_Keys.md)"
- Adjust scoring based on compound risk: Parent MEDIUM + Child MEDIUM = Combined HIGH

### 5. Challenge Assumptions
**Common false assumptions to avoid:**
- âŒ "SQL injection = Critical" (what if service has read-only DB access?)
- âŒ "Public endpoint = Internet accessible" (could be behind private endpoint with DNS)
- âŒ "Missing MFA = High risk" (on what? Admin portal vs health check endpoint)
- âŒ "Hard-coded secret = Exploitable" (what does the secret unlock? Is it rotated?)

**Instead ask:**

**CRITICAL SCORING PRINCIPLE:**
Score based on **actual exploitable damage given proven defenses**, not theoretical risk or principle violations.

âœ… **Correct scoring:**
- "SQL injection scored 5/10 - service has read-only DB permissions (confirmed in IaC), blast radius limited to data disclosure"
- "Unsigned JWT scored 6/10 - log poisoning confirmed, but APIM subscription keys (see terraform/apim.tf) block auth bypass"

âŒ **Incorrect scoring:**
- "SQL injection scored 9/10 because SQLi is inherently critical" (ignores blast radius)
- "Unsigned JWT scored 9/10 - violates security fundamentals" (ignores that damage is actually limited to log poisoning)

**Evidence requirements for defense layers:**
- Only credit defenses that are PROVEN (IaC files, repo findings, architecture diagrams with evidence)
- If defense is ASSUMED ("APIM probably validates"), flag in Validation Required and score WITHOUT the defense
- Cite evidence: "APIM JWT validation confirmed in terraform/apim_policies.tf line 45"

- âœ… "SQL injection on admin endpoint with DB owner role = Critical"
- âœ… "Public endpoint DNS record but NSG blocks public traffic = Low"
- âœ… "Missing MFA on Azure Portal for subscription owners = High"
- âœ… "Hard-coded API key for read-only public API = Low"

## Pre-Validation Side Effects (CRITICAL)

When reviewing authentication/authorization vulnerabilities, check what happens to untrusted data BEFORE validation:

**Common side effects to check:**

1. **Logging** - Is unvalidated user input written to logs?
   - Log injection: newlines, ANSI codes, fake entries
   - Information disclosure: sensitive data in logs before validation
   
2. **Metrics/Telemetry** - Are metrics recorded with unvalidated data?
   - Metric poisoning: fake success/failure counts
   - Cardinality attacks: exhaust metric storage
   
3. **Caching** - Is unvalidated data used as cache keys?
   - Cache poisoning: serve attacker's content to victims
   - Cache exhaustion: flood cache with fake keys
   
4. **Database Writes** - Are audit/tracking records written before validation?
   - Audit trail corruption
   - Database bloat attacks
   
5. **External Calls** - Are API calls made with unvalidated data?
   - Upstream service poisoning
   - Cost amplification (attacker triggers expensive API calls that fail)

**Example from FI_API_001:**
- TokenExtractionMiddleware logs InstitutionId â†’ Log injection possible
- AuthenticationMiddleware validates â†’ Main attack blocked
- **Both are true:** Compensating control works AND log injection works

**Scoring impact:**
- Main attack blocked by compensating control: Score reduced (e.g., 9/10 â†’ 5/10)
- Pre-validation side effect is exploitable: Note as separate finding or increase score (e.g., 5/10 â†’ 6/10 accounting for log tampering)

**âš ï¸ CRITICAL: Document First, Filter Later**

**Phase 2 Discovery Rule:**
- Document ALL security gaps, even with compensating controls
- Do NOT apply skeptic mindset during discovery phase
- Defense-in-depth violations are ALWAYS findings
- Skeptic reviews happen in Phase 3, AFTER findings are documented

**Defense-in-Depth Scoring:**
- **Without compensating controls:** MEDIUM-HIGH (6-8/10)
- **With compensating controls:** LOW-MEDIUM (3-5/10)
- **Example:** JWT signature not verified locally BUT validated by downstream service = 5/10 (still a finding, reduced severity)

**Pre-Validation Checklist**

Before concluding "not exploitable due to compensating control", check:

â˜ What happens to untrusted input BEFORE validation?  
â˜ Is it logged, cached, stored, or sent to external services?  
â˜ Can attacker poison logs, metrics, caches, or audit trails?  
â˜ Are there DoS amplification vectors (expensive operations before validation)?  
â˜ Does error handling leak information before validation?

**Pre-Validation Side Effect Remediation:**

**âš ï¸ DO NOT recommend removing logging/monitoring that happens before validation**
- Logging failed auth attempts is CRITICAL for security monitoring
- Attack detection requires seeing what attackers try (brute force, credential stuffing)
- Incident response needs full audit trail of access attempts
- Compliance often requires logging all requests

**âœ… CORRECT remediation for pre-validation logging:**
1. **Add rate limiting** - Prevent reconnaissance abuse via per-IP throttling
2. **Implement at edge** - Application Gateway/WAF-level throttling (preferred)
3. **Add alerting** - Detect patterns of failed auth attempts
4. **Keep logging** - Maintain security monitoring capability

**âŒ INCORRECT remediation:**
- Moving logging AFTER authentication (loses security visibility)
- Removing logging entirely (blind to attacks)
- Treating reconnaissance as binary problem (log vs don't log)

**The trade-off:** Rate limiting addresses volume/abuse. Logging addresses visibility. Both are needed.

## Hardcoded Values: Context Analysis

**âš ï¸ CRITICAL: Not all hardcoded values are security issues**

Before flagging a hardcoded value as a security finding, analyze:

**1. What is it?**
- Credential (API key, password, token, private key) â†’ **SECURITY ISSUE**
- Identifier (subscription ID, tenant ID, account number) â†’ **CONFIG ISSUE**
- Public metadata (region names, service names) â†’ **NOT AN ISSUE**

**2. What can you do with JUST this value?**
- âœ… Authenticate to a service â†’ **HIGH/CRITICAL**
- âœ… Access resources without additional auth â†’ **HIGH/CRITICAL**
- âœ… Sign/encrypt/decrypt data â†’ **MEDIUM/HIGH**
- âŒ Requires additional credentials to use â†’ **LOW/INFO**
- âŒ Publicly visible anyway (URLs, Portal) â†’ **INFO**

**3. Severity Guide:**

| Type | Example | Can Access? | Severity |
|------|---------|-------------|----------|
| **Credential** | API key, password, token | YES | ğŸ”´ CRITICAL/HIGH |
| **Connection string (with creds)** | `Server=x;User=sa;Password=123` | YES | ğŸ”´ HIGH |
| **Private key/certificate** | RSA private key, PFX with password | YES | ğŸ”´ HIGH |
| **Connection string (no creds)** | `Server=x` (uses managed identity) | NO | ğŸŸ¡ MEDIUM |
| **Architecture metadata** | Endpoint URLs, service names | NO | ğŸŸ¢ LOW |
| **Public identifiers** | Subscription ID, tenant ID, account # | NO | â„¹ï¸ INFO |

**4. Real-world examples:**

**ğŸ”´ SECURITY FINDING:**
```terraform
api_key = "sk-live-a1b2c3d4e5f6"  # Can authenticate to Stripe
password = "P@ssw0rd123"          # Can authenticate
```

**â„¹ï¸ CONFIG ISSUE (NOT security):**
```terraform
subscription_id = "8be5fe8d-..."  # Just an identifier, needs Azure AD auth
tenant_id = "12345678-..."        # Public metadata
account_id = "123456789012"       # AWS account # (public in ARNs)
```

**The rule:** Hardcoded values are SECURITY findings only if they grant authentication, authorization, or access without additional credentials.

## Context Sources

### Required Reading Before Scoring
1. **Architecture diagrams** (`Output/Summary/Cloud/Architecture_*.md`)
   - Request flow: Where does traffic enter? What's the path to the service?
   - Trust boundaries: Where does authentication happen?
   - Network isolation: Public, private, hybrid?

2. **Repo summaries** (`Output/Summary/Repos/`)
   - Middleware pipeline: What security controls execute before reaching vulnerable code?
   - Authentication patterns: JWT, OAuth, API keys, managed identity?
   - Service permissions: What can this service access?

3. **Knowledge files** (`Output/Knowledge/`)
   - Confirmed controls: WAF, Private Endpoints, Defender, network policies
   - Environment tier: Production vs non-production
   - Deployment patterns: Bastion, JIT, VPN access

## Behaviour
- Follow `Agents/Instructions.md` and `Settings/Styling.md`.
- Ask for missing context when required (cloud provider, environment, exposure).
- Review `Knowledge/` sources for confirmed environment and code facts.
- Review architecture Mermaid diagrams in `Summary/Cloud/Architecture_*.md` to
  understand service context and trust boundaries.
- Use the relevant template:
  - `Templates/CloudFinding.md`
  - `Templates/CodeFinding.md`
- Keep scores consistent with the repoâ€™s severity mapping in `Settings/Styling.md`.
- Recommend prevention-oriented controls where appropriate (e.g., guardrails/policy-as-code, secure-by-default baselines) and pair them with developer-executable fixes (code/config changes).
- Be appropriately sceptical and look for current countermeasures that reduce
- **Challenge vendor-assigned severity:** Cloud provider advisories often over-prioritize findings to upsell services. Assess ACTUAL risk vs recommended priority:
  - Check for **compensating controls** (3rd party tools, existing defenses)
  - Distinguish **genuine gaps** from **service upsells**
  - Azure Defender/AWS GuardDuty/GCP SCC recommendations are often "enable paid tier" not "you have a vulnerability"
  - Question high-severity ratings on recommendations for premium services (DDoS Standard, Advanced Threat Protection, etc.)
  risk. If present, document them with reasoning and downscore the risk.
- Listen to Dev and Platform skeptic feedback and incorporate valid points.
- Ensure the finding summary is understandable to non-specialists.

## Writing the Exploitability Section

**The `### ğŸ¯ Exploitability` section MUST include realistic attack path analysis:**

### Structure:
```markdown
### ğŸ¯ Exploitability

**Attack Prerequisites:**
- Network: [Internet / VPN / Internal network / Cloud console access]
- Authentication: [Anonymous / Valid user / Specific role / Admin credentials]
- Knowledge: [Public documentation / Service discovery / Source code access]

**Attack Path:**
1. [Step 1: How attacker gains initial access]
2. [Step 2: Traversing security controls/boundaries]
3. [Step 3: Reaching the vulnerable component]
4. [Step 4: Exploiting the vulnerability]
5. [Step 5: Achieving impact (data exfil, lateral movement, etc.)]

**Defense Layers Encountered:**
- âœ… [Control name]: [How it affects exploitation]
- âš ï¸ [Missing control]: [Gap that enables exploitation]

**Example Scenario:**
[Concrete example with real attacker steps]

**Exploitation Difficulty:** [Easy/Medium/Hard/Very Hard]
**Reason:** [Why - based on prerequisites, defense layers, technical complexity]
```

### Example - Good Exploitability Section:
```markdown
**Attack Prerequisites:**
- Network: Internet access (FI API has public App Service endpoint)
- Authentication: Valid JWT token from any FI client
- Knowledge: API endpoint structure (documented in OpenAPI spec)

**Attack Path:**
1. Attacker with compromised FI client credentials obtains valid JWT
2. Sends crafted request directly to FI API public endpoint
3. Request passes through middleware pipeline (logging, token validation)
4. Reaches vulnerable endpoint with malicious payload
5. Exploits vulnerability to [impact]

**Defense Layers Encountered:**
- âœ… JWT validation: Requires valid token (prevents anonymous exploitation)
- âœ… Rate limiting: 100 req/min via APIM (slows automated attacks)
- âš ï¸ Input validation: Missing on X parameter (enables injection)
- âœ… Network isolation: VNet integration limits lateral movement

**Example Scenario:**
A financial institution employee with legitimate API access sends a crafted JSON payload...

**Exploitation Difficulty:** Medium
**Reason:** Requires valid JWT token (not publicly accessible) but no additional authorization checks on vulnerable endpoint.
```

### Example - Bad Exploitability Section (Don't Do This):
```markdown
**Exploitability:**
An attacker could exploit this SQL injection vulnerability by sending malicious input.
This is a critical risk.
```
**Why bad:** No attack path, no prerequisites, no defense layers, no realistic scenario.

## Proof of Concept (When Applicable)

**Include a POC for exploitable findings:**

Required elements:
1. **Single executable script** - Developers should copy/paste and run
2. **Real endpoints/resources** - Use actual paths from the scanned repo/cloud
3. **Minimal configuration** - One URL/resource ID to change
4. **Expected results** - What to see before and after fix
5. **Verification steps** - How to check logs/database/behavior

**When to include POC:**
- âœ… Injection attacks (SQL, XSS, command, log)
- âœ… Auth bypasses (can demonstrate with curl)
- âœ… Public resources (show download/access)
- âœ… Network misconfigurations (show connection)
- âŒ Purely theoretical findings
- âŒ Requires >10 setup steps
- âŒ Needs privileged access dev team doesn't have

**POC Template:**
```markdown
## ğŸ§ª Proof of Concept

**Prerequisites:**
- [What's needed: access, tools, endpoints]

### Complete Test Script (Copy & Run)

```bash
#!/bin/bash
# [Purpose of this test]

# CONFIGURE YOUR ENVIRONMENT
RESOURCE_URL="https://[change-this]"

# Step 1: [Action description]
[commands with actual endpoints/resources]

echo "Expected: [result]"
```

### Verify Impact
[How to see the exploit worked - logs, database, behavior]

### Test the Fix
```bash
# After applying recommended fix
[Same commands - should now be blocked/secured]
```

**Expected after fix:** [Secure behavior]
```

## Deliverables per triage
- A new/updated finding file under `Findings/`.
- Any confirmed facts added to `Knowledge/`.
- Any impacted summaries updated under `Summary/`.

### Repo Scan Finding Extraction (MANDATORY)

**When scanning repos, findings are often initially documented in the repo summary's "Security Observations" section. These MUST be extracted as individual finding files.**

**Extraction criteria:**
- Extract **all MEDIUM+ severity findings** as individual files
- Extract **HIGH/CRITICAL findings** even if marked as INFO due to mitigations (document both states)
- Leave **LOW/INFO findings** in repo summary only (unless high business impact)

**Extraction workflow:**
1. **Review repo summary** `Output/Summary/Repos/<RepoName>.md` â†’ "Security Observations" section
2. **For each MEDIUM+ finding**, create `Output/Findings/Code/<RepoName>_<Issue>_<Number>.md`:
   - Copy architecture context from repo summary
   - Copy finding details (location, issue, attack vector, mitigations)
   - Add POC script section (if exploitable)
   - Add blank Skeptic sections
   - Add metadata (source: repo scan, resource: <RepoName>)
3. **Run skeptic reviews** on each extracted finding file
4. **Link findings** back to repo summary under "Compounding Findings"

**Example:**
```
Repo summary has: "Pre-Validation Logging Side Effect (MEDIUM)"
â†’ Extract to: Output/Findings/Code/FI_API_001_Pre_Validation_Logging.md
â†’ Include POC showing path enumeration attack
â†’ Run Dev + Platform skeptics
â†’ Link in repo summary: [FI_API_001](../../Findings/Code/FI_API_001_Pre_Validation_Logging.md)
```

**Why this matters:**
- Individual findings can be tracked in risk register
- Findings can be linked across repos
- POC scripts are discoverable per finding
- Remediation status tracked independently

### Finding Structure Requirements
- **TL;DR - Executive Summary:** After Dev and Platform Skeptic reviews are complete, add a `## ğŸ“Š TL;DR - Executive Summary` section immediately after the architecture diagram. This gives security engineers immediate visibility into:
  - Final score with adjustment tracking (Security Review â†’ Dev â†’ Platform)
  - Top 3 priority actions with effort estimates
  - Material risks summary (2-3 sentences)
  - Why the score changed (if Dev/Platform adjusted it)
- **Validation Required:** If there are **critical unconfirmed assumptions** that could significantly change the risk score, add a `## â“ Validation Required` section immediately after the TL;DR. This must:
  - Clearly state what was assumed and why it matters
  - Show evidence found vs evidence NOT found
  - Explain impact on score if assumption is confirmed/rejected
  - Ask a specific question for the human reviewer
  - Common critical assumptions: network ingress paths, public vs private access, authentication mechanisms, blast radius

### Risk Register Content Requirements
- **Summary:** Must be meaningful business impact statement, not generic boilerplate
- **Key Evidence:** Include specific resource IDs, paths, or service names for accurate resource type classification
- **Applicability:** Clear status with specific evidence helps establish scope and priority
- The risk register generator uses these sections to classify resource types and extract issues

## Common Cloud Provider Upsells to Challenge

### Azure Advisor / Defender Recommendations

**High-priority upsells (often marked "High" but may be low actual risk):**

1. **"Enable Azure Defender for [Service]"**
   - **What it is:** Paid threat detection service ($15-30/month per resource type)
   - **Check before scoring high:**
     - Do you have 3rd party EDR/XDR (CrowdStrike, SentinelOne)?
     - Do you have SIEM with cloud log ingestion (Splunk, Sentinel)?
     - Is threat detection required by compliance framework?
   - **Actual risk if not enabled:** LOW-MEDIUM (depends on existing detection stack)
   - **Recommended score:** 3-5/10 unless no other detection exists (then 6-7/10)

2. **"Enable DDoS Protection Standard"**
   - **What it is:** $2,944/month for advanced DDoS protection
   - **Check before scoring high:**
     - Are services behind WAF/CDN with DDoS protection (Cloudflare, Akamai)?
     - Is this internet-facing or internal?
     - What's the attack surface (single IP vs many)?
   - **Actual risk if not enabled:** LOW for internal, MEDIUM for internet-facing with CDN, HIGH for exposed without CDN
   - **Recommended score:** 2-4/10 with CDN, 6-8/10 without any DDoS protection on public IPs

3. **"Enable Advanced Threat Protection for Storage/SQL"**
   - **What it is:** Paid anomaly detection for unusual access patterns
   - **Check before scoring high:**
     - Do you have baseline monitoring/alerting in place?
     - Is this production data or test data?
     - What's the data sensitivity?
   - **Actual risk if not enabled:** LOW-MEDIUM (nice-to-have, not critical)
   - **Recommended score:** 3-5/10

### AWS Security Hub / GuardDuty Recommendations

**High-priority upsells:**

1. **"Enable GuardDuty"**
   - **What it is:** Paid threat detection ($4.50/month + usage-based)
   - **Check:** Same as Azure Defender - do you have other detection tools?
   - **Recommended score:** 3-6/10

2. **"Enable Security Hub Premium"**
   - **What it is:** Compliance dashboards and aggregated findings
   - **Actual risk if not enabled:** NONE (it's a dashboard, not a control)
   - **Recommended score:** 2-3/10 (operational convenience, not security gap)

### GCP Security Command Center Recommendations

**High-priority upsells:**

1. **"Upgrade to Security Command Center Premium"**
   - **What it is:** Paid threat detection and compliance dashboard
   - **Check:** Same pattern as above
   - **Recommended score:** 3-6/10

## Assessment Framework for Cloud Recommendations

When triaging a cloud advisory/recommendation:

1. **Identify if it's an upsell:**
   - Does it recommend enabling a PAID service/tier?
   - Is the finding essentially "you haven't bought product X"?

2. **Check for compensating controls:**
   - 3rd party security tools (EDR, SIEM, CSPM, WAF, CDN)
   - Native logging + custom alerting
   - **IMPORTANT:** Compensating controls reduce severity but DO NOT eliminate the finding
   - Defense-in-depth violations must still be documented (LOW-MEDIUM with controls, MEDIUM-HIGH without)
   - Network isolation/private endpoints
   - Defense-in-depth layers

3. **Assess ACTUAL risk:**
   - What attack scenario does this prevent?
   - Is that scenario realistic given your architecture/exposure?
   - What's the likelihood vs impact?

4. **Re-score based on reality:**
   ```markdown
   ## ğŸ“Š Vendor vs Actual Severity
   
   - **Vendor Severity:** High (Azure Advisor)
   - **Vendor Recommendation:** Enable Azure Defender for Storage ($15/month per account)
   - **Actual Risk Assessment:** LOW-MEDIUM (3/10)
   - **Rationale:** 
     - Storage accounts use private endpoints (not internet-exposed)
     - SIEM (Splunk) ingests storage logs with custom alert rules
     - No compliance requirement for Defender specifically
     - This is a "nice-to-have" threat detection upsell, not a critical gap
   - **Recommendation:** Defer unless compliance requires or budget allows. Current monitoring adequate for threat level.
   ```

5. **Document in finding:**
   - Note vendor-assigned severity vs your assessment
   - List compensating controls
   - Explain why you downgraded (or kept) the score
   - Provide cost context if it's a paid service

## Examples

**Example 1: Defender recommendation downgraded**
> **Finding:** "Enable Azure Defender for App Service (Vendor: High)"
> 
> **Compensating Controls:** CrowdStrike EDR on all compute, Sentinel SIEM with App Service log ingestion, WAF in front of all public endpoints.
> 
> **Actual Severity:** LOW (3/10) - Defender provides marginal additional value given existing detection stack. This is an upsell, not a critical gap.

**Example 2: DDoS Standard kept high**
> **Finding:** "Enable DDoS Protection Standard (Vendor: High)"
> 
> **Compensating Controls:** NONE - direct public IPs on VMs, no CDN, no 3rd party DDoS protection.
> 
> **Actual Severity:** HIGH (8/10) - No DDoS protection for internet-facing infrastructure. Basic tier insufficient for sustained attacks. This is a genuine gap, not just upsell.

**Example 3: Advanced Threat Protection downgraded**
> **Finding:** "Enable Advanced Threat Protection for SQL (Vendor: High)"
> 
> **Compensating Controls:** SQL databases use private endpoints (not internet-accessible), database audit logs feed into SIEM with anomaly detection rules, JIT/Bastion for admin access.
> 
> **Actual Severity:** MEDIUM (4/10) - ATP provides SQL-specific anomaly detection, but private networking + monitoring reduce attack surface and provide baseline detection. Nice-to-have for defense-in-depth, not critical.

## Resource-Specific Security Checklists

### Azure Storage Account Checklist

When reviewing storage accounts (especially those containing sensitive data like logs, dumps, backups), check these specific settings:

| Risk | Terraform Setting | Insecure | Secure | Severity if Insecure |
|------|-------------------|----------|--------|---------------------|
| **Public blob access** | `allow_blob_public_access` | `true` | `false` | ğŸ”´ Critical |
| **Anonymous container** | Container access level | `blob`/`container` | `private` | ğŸ”´ Critical |
| **Storage keys enabled** | `shared_access_key_enabled` | `true` | `false` | ğŸŸ  High |
| **Network open** | `network_rules.default_action` | `Allow` | `Deny` | ğŸŸ  High |
| **HTTP allowed** | `enable_https_traffic_only` | `false` | `true` | ğŸŸ  High |
| **No private endpoint** | `private_endpoint_connection` | missing | configured | ğŸŸ  High |
| **No retention policy** | `lifecycle_management` | missing | configured | ğŸŸ¡ Medium |
| **Soft delete off** | `blob_soft_delete_retention_days` | `0` | `7-365` | ğŸŸ¡ Medium |

**Access method priority (most to least secure):**
1. âœ… Managed Identity (no credentials to leak)
2. ğŸŸ¡ User delegation SAS (tied to AAD identity)
3. ğŸŸ  Service SAS (time-limited but leakable)
4. ğŸ”´ Account SAS (broad access, leakable)
5. â›” Storage account keys (full access, long-lived)

**Red flags for sensitive data storage:**
- Memory dumps, crash logs â†’ secrets in memory
- Backup storage â†’ full database contents
- Log storage â†’ potential PII, tokens in logs
- Terraform state â†’ secrets in state file

### Azure App Service Checklist

| Risk | Terraform Setting | Insecure | Secure |
|------|-------------------|----------|--------|
| **HTTP allowed** | `https_only` | `false` | `true` |
| **Old TLS** | `minimum_tls_version` | `"1.0"`/`"1.1"` | `"1.2"` |
| **FTP enabled** | `ftps_state` | `"AllAllowed"` | `"Disabled"` |
| **No VNet integration** | `vnet_route_all_enabled` | `false` | `true` |
| **No IP restrictions** | `ip_restriction` | missing | configured |
| **Managed identity off** | `identity.type` | missing | `"SystemAssigned"` |

### Azure Key Vault Checklist

| Risk | Terraform Setting | Insecure | Secure |
|------|-------------------|----------|--------|
| **Public network access** | `public_network_access_enabled` | `true` | `false` |
| **No private endpoint** | `private_endpoint_connection` | missing | configured |
| **Soft delete off** | `soft_delete_retention_days` | `0` | `7-90` |
| **Purge protection off** | `purge_protection_enabled` | `false` | `true` |
| **RBAC not used** | `enable_rbac_authorization` | `false` | `true` |

---

## Data Classification Framework

**CRITICAL:** Security findings MUST be assessed with data classification context. The same misconfiguration has vastly different severity based on data sensitivity.

### Data Sensitivity Tiers

```
ğŸ”´ TIER 1: REGULATED DATA (Compliance-Driven)
â”œâ”€â”€ Payment Card Data (PCI-DSS) - 16-digit PANs, CVV, cardholder name
â”œâ”€â”€ Protected Health Information (PHI/HIPAA) - Medical records, diagnosis, treatment
â”œâ”€â”€ Government IDs - SSN, passport numbers, driver's license
â”œâ”€â”€ Authentication Credentials - Passwords, private keys, API tokens, certificates
â””â”€â”€ Biometric Data - Fingerprints, facial recognition, DNA

ğŸŸ  TIER 2: PERSONAL DATA (Privacy-Driven)
â”œâ”€â”€ Personally Identifiable Information (PII/GDPR) - Email, phone, address, DOB
â”œâ”€â”€ Financial Information - Bank accounts, salary, credit scores
â”œâ”€â”€ Demographic Data - Race, religion, political affiliation, sexual orientation
â”œâ”€â”€ Communication Data - Emails, chat logs, call recordings
â””â”€â”€ Behavioral Data - Browsing history, location tracking, purchase history

ğŸŸ¡ TIER 3: BUSINESS CONFIDENTIAL (Commercial Risk)
â”œâ”€â”€ Trade Secrets - Algorithms, formulas, source code
â”œâ”€â”€ Customer Lists - CRM data, contacts, contracts
â”œâ”€â”€ Financial Records - Revenue, costs, margins, forecasts
â”œâ”€â”€ Strategic Plans - M&A targets, product roadmaps
â””â”€â”€ Internal Communications - Executive emails, board minutes

ğŸŸ¢ TIER 4: INTERNAL USE (Limited Risk)
â”œâ”€â”€ Employee Directories - Names, titles, org charts
â”œâ”€â”€ Operational Metrics - System performance, uptime stats
â”œâ”€â”€ Public Marketing Material - Whitepapers, blog posts
â””â”€â”€ Aggregate Analytics - Anonymized usage statistics

âšª TIER 5: PUBLIC DATA (No Risk)
â”œâ”€â”€ Open Source Code - GitHub public repos
â”œâ”€â”€ Published Content - Documentation, press releases
â”œâ”€â”€ Synthetic Test Data - Faker-generated records
â””â”€â”€ Anonymized Datasets - No re-identification risk
```

### Detection Strategy

**Phase 1: Infrastructure Hints (IaC Analysis)**

Look for data classification signals in Terraform/IaC:

```python
# Database names
payment|card|billing|invoice|stripe|checkout|transaction  # â†’ TIER 1 (PCI)
patient|medical|health|diagnosis|prescription|hipaa        # â†’ TIER 1 (PHI)
user|customer|account|profile|contact|member|email         # â†’ TIER 2 (PII)
credential|secret|key|token|password|auth                  # â†’ TIER 1 (Auth)

# Table/Container names in SQL scripts or ARM templates
CREATE TABLE customers (email, phone, address)             # â†’ TIER 2 (PII)
CREATE TABLE payments (card_number, cvv, expiry)          # â†’ TIER 1 (PCI)
CREATE TABLE audit_logs (timestamp, user_id, action)      # â†’ TIER 4 (Internal)

# Azure/AWS/GCP resource tags
data-classification = "confidential"                       # â†’ TIER 3
contains-pii = "true"                                      # â†’ TIER 2
compliance-scope = "pci-dss"                               # â†’ TIER 1
```

**Phase 2: Code Analysis (Application Review)**

Look for actual data handling patterns:

```javascript
// API endpoints that collect sensitive data
POST /api/register â†’ { email, password }                  # â†’ TIER 2 + TIER 1
POST /api/payment â†’ { card_number, cvv }                  # â†’ TIER 1 (PCI)
GET /api/health-records â†’ { diagnosis, medications }      # â†’ TIER 1 (PHI)

// Database queries
INSERT INTO users (email, phone, address)                 # â†’ TIER 2 (PII)
SELECT * FROM payments WHERE card_number LIKE            # â†’ TIER 1 (PCI)
```

**Phase 3: Data Flow Mapping**

Trace data from ingress â†’ storage â†’ egress:

```
Example: E-commerce application

Internet â†’ App Service (collects: email, card_number)
  â”œâ”€ TIER 2: email â†’ SQL Database (users table)
  â””â”€ TIER 1: card_number â†’ Payment Gateway (Stripe API)
```

### Severity Modifiers Based on Data Classification

| Data Tier | Base Finding Severity | Modifier | Example |
|-----------|----------------------|----------|---------|
| **TIER 1** | Any misconfiguration | Auto-escalate to CRITICAL (9-10/10) | SQL no encryption + PCI data = 10/10 |
| **TIER 2** | HIGH (7-8) | +1 point | Public blob + PII = 8/10 â†’ 9/10 |
| **TIER 3** | MEDIUM (5-6) | +0 points | Trade secrets + weak access = 6/10 |
| **TIER 4** | LOW (3-4) | +0 points | Internal metrics + no auth = 4/10 |
| **TIER 5** | Any misconfiguration | -2 points (min 3/10) | Public test data + no encryption = 3/10 |

**Compliance Requirements by Data Tier:**

| Data Tier | Required Encryption | Required Logging | Required Network Isolation | Key Management |
|-----------|-------------------|------------------|---------------------------|----------------|
| **TIER 1** | âœ… At rest + in transit (TLS 1.2+) | âœ… All operations, 90+ days | âœ… Private endpoints, no public access | âœ… CMK (Customer-Managed Keys) |
| **TIER 2** | âœ… At rest + in transit (TLS 1.2+) | âœ… All operations, 90+ days | âš ï¸ Network restrictions required | ğŸŸ¡ CMK recommended |
| **TIER 3** | âœ… At rest + in transit | âœ… Access logs, 30+ days | ğŸŸ¡ Recommended | ğŸŸ¡ Platform-managed OK |
| **TIER 4** | ğŸŸ¡ Recommended | ğŸŸ¡ Recommended | âšª Not required | âšª Any |
| **TIER 5** | âšª Not required | âšª Not required | âšª Not required | âšª Any |

### Applying Data Classification to Findings

**In every finding's Security Review section, include:**

```markdown
### ğŸ—‚ï¸ Data Classification

**Primary Data Type:** TIER 2 - Personal Data (PII)
- **Detected from:** SQL schema analysis (users table: email, phone, address)
- **Evidence:** `terraform/sql/init.sql:15-23`
- **Compliance Scope:** GDPR Article 32 (Security of processing)

**Severity Impact:**
- Base Score: 6/10 (SQL database with overly broad firewall)
- Data Classification Modifier: +1 (TIER 2 PII)
- **Final Score: 7/10 HIGH**

**Rationale:** PII exposure increases breach notification requirements (GDPR Art 33: 72 hours) and potential fines (â‚¬20M or 4% revenue).
```

**For TIER 1 data, always include compliance requirements:**

```markdown
### ğŸ—‚ï¸ Data Classification

**Primary Data Type:** TIER 1 - Payment Card Data (PCI-DSS)
- **Detected from:** Database table schema (payments: card_number, cvv, expiry)
- **Evidence:** `terraform/sql/payments_schema.sql:8-12`
- **Compliance Scope:** PCI-DSS Requirement 3 (Protect stored cardholder data)

**Mandatory Controls for TIER 1:**
- âœ… Encryption at rest with CMK (PCI Req 3.4)
- âŒ **MISSING:** TLS 1.2+ enforcement (PCI Req 4.1)
- âŒ **MISSING:** Access logging enabled (PCI Req 10.2)
- âŒ **MISSING:** Network segmentation (PCI Req 1.3)

**Severity Impact:**
- Auto-escalate to CRITICAL: TIER 1 data + missing encryption = **10/10 CRITICAL**
- Non-compliance penalty: Loss of PCI certification + merchant account termination
```

### Unknown Data Classification

**When data classification cannot be determined:**

```markdown
### ğŸ—‚ï¸ Data Classification

**Primary Data Type:** UNKNOWN (Validation Required)
- **Evidence found:** Generic database name "app-db", no schema visible
- **Assumed:** TIER 3 (Business Confidential) for scoring purposes
- **Requires validation:** Database schema inspection or application code review

**Impact on Score:**
- If TIER 1/2: Score could escalate from 6/10 â†’ 8-9/10
- If TIER 5 (test data): Score could reduce to 4/10

**Validation Required:**
- [ ] Inspect database schema or sample 10 records
- [ ] Review application code for data collection patterns
- [ ] Check for compliance tags (PCI, HIPAA, GDPR)
```

### Data Classification Decision Tree

```
Is actual data content visible? (schema, API responses)
â”œâ”€ YES â†’ Classify by content
â”‚   â”œâ”€ Contains PCI/PHI/credentials? â†’ TIER 1
â”‚   â”œâ”€ Contains PII (email/phone/address)? â†’ TIER 2
â”‚   â”œâ”€ Contains business confidential? â†’ TIER 3
â”‚   â”œâ”€ Contains internal/operational? â†’ TIER 4
â”‚   â””â”€ Synthetic/public data? â†’ TIER 5
â”‚
â””â”€ NO â†’ Classify by context clues
    â”œâ”€ Resource name contains "payment/card/billing"? â†’ Assume TIER 1
    â”œâ”€ Resource name contains "user/customer/profile"? â†’ Assume TIER 2
    â”œâ”€ Resource tagged "confidential"? â†’ Assume TIER 3
    â”œâ”€ Resource name generic (e.g., "app-db")? â†’ Assume TIER 3 (default)
    â””â”€ Resource name contains "test/demo/sample"? â†’ Assume TIER 5

If classification uncertain:
â†’ Flag in "Validation Required" section
â†’ Score using TIER 3 (middle tier) as conservative default
â†’ Note how score would change if TIER 1/2 confirmed
```

### Example: Data Classification in Practice

**Scenario:** ExpanseAzureLab SQL Database

**Evidence:**
```sql
-- From terraform/sql/init.sql
CREATE TABLE users (
    user_id INT PRIMARY KEY,
    email VARCHAR(255),        -- TIER 2: PII
    username VARCHAR(100),      -- TIER 2: PII
    service_principal_id GUID   -- TIER 1: Credential reference
);
```

**Classification:**
- **Primary:** TIER 2 (PII - emails, usernames)
- **Secondary:** TIER 1 (Service principal references)
- **Overall Tier:** TIER 1 (use highest sensitivity)

**Impact on Finding SQL_Firewall_Allows_Azure_Services:**
- Base Score: 5/10 (overly broad firewall rule)
- Data Classification: TIER 1 (credentials + PII)
- **Final Score: 8/10 HIGH** (auto-escalated due to TIER 1)

---

## The Five Pillars Security Framework

**CRITICAL:** Security assessment MUST systematically check ALL FIVE pillars for EVERY resource. Missing any pillar = incomplete security assessment.

For EVERY IaC resource detected:

1ï¸âƒ£ **Network Security** - Can they reach it?
   - Network ACLs / Firewall rules configured?
   - Private endpoints used?
   - Public access disabled?
   
2ï¸âƒ£ **Access Control** - Can they authenticate/authorize?
   - Modern authentication (Managed Identity, AAD, IAM)?
   - RBAC/IAM least privilege?
   - Legacy auth disabled (keys, passwords)?
   - JIT access / credential expiry?
   
3ï¸âƒ£ **Audit Logging** - Are events logged?
   - Audit logging enabled?
   - Comprehensive event coverage?
   - Retention meets compliance (90+ days)?
   
4ï¸âƒ£ **Log Consumption** - Is anyone watching?
   - Centralized logging infrastructure?
   - SIEM deployed (Sentinel, GuardDuty, etc)?
   - Security alerts configured?
   - Response process defined?

5ï¸âƒ£ **Data Protection** - Is data encrypted and secure?
   - Encryption at rest enabled?
   - TLS/HTTPS enforced (1.2+ minimum)?
   - Customer-managed keys (CMK) for compliance?
   - Key rotation configured?

**Security Score = MIN(all five pillars)** - Weakest link determines overall security.

### Pillar 1: Network Security Matrix

| Azure Resource | Network Feature | Property to Check | Default Behavior | Risk if Missing |
|----------------|-----------------|-------------------|------------------|-----------------|
| **Key Vault** | Network ACLs | `network_acls.default_action` | Allow | ğŸŸ  HIGH |
| **Storage Account** | Network rules | `network_rules.default_action` | Allow | ğŸŸ  HIGH |
| **SQL Server** | Firewall rules | `firewall_rule` | None (blocked) | ğŸŸ¡ MEDIUM |
| **AKS** | API server auth IPs | `api_server_authorized_ip_ranges` | All IPs | ğŸŸ  HIGH |
| **App Service** | IP restrictions | `ip_restriction` | None (open) | ğŸŸ  HIGH |
| **Virtual Machine** | NSG rules | `network_security_group_id` | None | ğŸ”´ CRITICAL |
| **PostgreSQL** | Firewall rules | `firewall_rule` | None (blocked) | ğŸŸ¡ MEDIUM |
| **MySQL** | Firewall rules | `firewall_rule` | None (blocked) | ğŸŸ¡ MEDIUM |
| **Cosmos DB** | IP firewall | `ip_range_filter` | None (open) | ğŸŸ  HIGH |
| **Redis Cache** | Firewall rules | `firewall_rule` | None (open) | ğŸŸ  HIGH |
| **Container Registry** | Network rules | `network_rule_set` | Allow all | ğŸŸ¡ MEDIUM |
| **Function App** | IP restrictions | `ip_restriction` | None (open) | ğŸŸ  HIGH |
| **API Management** | Virtual network | `virtual_network_type` | None (external) | ğŸŸ¡ MEDIUM |
| **Event Hub** | Network rules | `network_rulesets` | Allow all | ğŸŸ¡ MEDIUM |
| **Service Bus** | Network rules | `network_rule_set` | Allow all | ğŸŸ¡ MEDIUM |

**AWS Equivalent Resources:**
- S3: `bucket_public_access_block`
- RDS: `publicly_accessible`
- EC2: `security_group_rule`
- Lambda: `vpc_config`

**GCP Equivalent Resources:**
- Cloud Storage: `uniform_bucket_level_access`
- Cloud SQL: `ip_configuration.authorized_networks`
- Compute Engine: `firewall_rule`

### Pillar 2: Access Control Matrix

| Azure Resource | Best Practice Auth | Legacy Auth to Flag | Property to Check | Risk if Legacy |
|----------------|-------------------|---------------------|-------------------|----------------|
| **Storage Account** | Managed Identity | Account keys | `shared_access_key_enabled` = true | ğŸŸ  HIGH |
| **Key Vault** | AAD RBAC | Access policies | `enable_rbac_authorization` = false | ğŸŸ¡ MEDIUM |
| **SQL Server** | AAD authentication | SQL logins | `azurerm_mssql_server_aad_administrator` missing | ğŸŸ  HIGH |
| **AKS** | AAD integration | Basic auth | `azure_active_directory_role_based_access_control` missing | ğŸŸ  HIGH |
| **Virtual Machine** | SSH keys + AAD | Password auth | `disable_password_authentication` = false | ğŸ”´ CRITICAL |
| **App Service** | Managed Identity | Connection strings | `identity` block missing | ğŸŸ¡ MEDIUM |
| **Service Principal** | Credential expiry | No expiry | `end_date` missing or >> 1 year | ğŸŸ  HIGH |
| **PostgreSQL** | AAD auth | Password only | `azurerm_postgresql_aad_administrator` missing | ğŸŸ¡ MEDIUM |
| **MySQL** | AAD auth | Password only | `azurerm_mysql_aad_administrator` missing | ğŸŸ¡ MEDIUM |
| **Cosmos DB** | RBAC | Connection strings | `key_vault_key_id` missing + keys enabled | ğŸŸ  HIGH |
| **Container Registry** | Managed Identity | Admin user | `admin_enabled` = true | ğŸŸ  HIGH |
| **API Management** | Subscription + JWT | API keys only | Policy validation | ğŸŸ¡ MEDIUM |
| **Virtual Machine** | JIT access | Always open RDP/SSH | `azurerm_security_center_jit` missing | ğŸ”´ CRITICAL |
| **Function App** | Managed Identity | App keys | `identity` block missing | ğŸŸ¡ MEDIUM |
| **Storage Account** | User delegation SAS | Account SAS | SAS policy configuration | ğŸŸ¡ MEDIUM |

**Authentication Priority (most to least secure):**
1. âœ… Managed Identity (no credentials)
2. âœ… AAD + RBAC (centralized, auditable)
3. ğŸŸ¡ SSH keys + expiry (time-limited)
4. ğŸŸ  Connection strings in Key Vault (leakable but protected)
5. ğŸ”´ Passwords / Account keys (long-lived, leakable)
6. â›” No authentication (anonymous access)

### Pillar 3: Audit Logging Matrix

| Azure Resource | Logging Feature | Property to Check | What's Logged | Retention Requirement |
|----------------|-----------------|-------------------|---------------|----------------------|
| **Key Vault** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | Secret access, key ops | 90+ days (compliance) |
| **Storage Account** | Storage logging | `logging` block | Blob/Queue/Table ops | 90+ days |
| **SQL Server** | Auditing | `azurerm_mssql_server_extended_auditing_policy` | All DB operations | 90+ days |
| **AKS** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | K8s API, kubelet logs | 30+ days |
| **App Service** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | HTTP logs, app logs | 30+ days |
| **Virtual Machine** | Diagnostic extension | `azurerm_virtual_machine_extension` | System logs, security events | 90+ days |
| **NSG** | Flow logs | `azurerm_network_watcher_flow_log` | Network traffic patterns | 30+ days |
| **PostgreSQL** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | Query logs, connection logs | 90+ days |
| **MySQL** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | Query logs, audit logs | 90+ days |
| **Cosmos DB** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | Data plane operations | 90+ days |
| **API Management** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | API calls, gateway logs | 90+ days |
| **Function App** | Application Insights | `application_insights_connection_string` | Invocations, dependencies | 90+ days |
| **Subscription** | Activity Log | Automatic (check export) | Management operations | 90+ days |
| **Virtual Network** | NSG flow logs | `azurerm_network_watcher_flow_log` | Traffic flows | 30+ days |
| **Firewall** | Diagnostic settings | `azurerm_monitor_diagnostic_setting` | Allowed/denied traffic | 90+ days |

**Critical Configuration:**
- Logs MUST go to Log Analytics Workspace (centralized)
- Retention: 90+ days for compliance (GDPR, SOC2, PCI-DSS)
- Enable ALL log categories (not just errors)

**AWS/GCP Equivalents:**
- AWS: CloudTrail, VPC Flow Logs, CloudWatch Logs
- GCP: Cloud Audit Logs, VPC Flow Logs, Cloud Logging

### Pillar 4: Log Consumption & Monitoring Infrastructure

**Environment-Level Assessment (Not per-resource):**

| Component | Azure | AWS | GCP | Risk if Missing |
|-----------|-------|-----|-----|-----------------|
| **Central Logging** | Log Analytics Workspace | CloudWatch Logs | Cloud Logging | ğŸ”´ CRITICAL |
| **SIEM** | Azure Sentinel | Amazon GuardDuty | Security Command Center | ğŸ”´ CRITICAL |
| **Alerting** | Azure Monitor Alerts | CloudWatch Alarms | Cloud Monitoring | ğŸŸ  HIGH |
| **Security Dashboards** | Sentinel Workbooks | Security Hub | SCC Dashboards | ğŸŸ¡ MEDIUM |
| **Incident Response** | Logic Apps / Playbooks | Lambda + SNS | Cloud Functions | ğŸŸ¡ MEDIUM |

**Monitoring Maturity Levels:**

- **Level 0 (Blind):** No centralized logging, no SIEM, no alerts
  - **Risk:** Breaches undetected for months/years
  - **Finding:** ENV-001 - No Security Monitoring Infrastructure

- **Level 1 (Reactive):** Centralized logging exists, but no one watches
  - **Risk:** Logs exist but breaches still undetected
  - **Finding:** ENV-002 - Logs Not Consumed by SIEM

- **Level 2 (Alerted):** SIEM deployed, basic alerts configured
  - **Risk:** Alert fatigue, false positives
  - **Finding:** Review alert tuning and response SLAs

- **Level 3 (Responsive):** Alerts tuned, incident response playbooks active
  - **Risk:** Manual response delays
  - **Finding:** Validate response times

- **Level 4 (Automated):** Automated response, threat hunting active
  - **Risk:** Minimal - monitor for gaps
  - **Finding:** Continuous improvement opportunities

**Red Flags:**
- Logs going to storage account only (no consumption)
- Log Analytics workspace empty (no data sources connected)
- Sentinel deployed but no analytics rules enabled
- Alerts send to unused email aliases
- No runbooks or playbooks configured

### Pillar 5: Data Protection Matrix

| Azure Resource | Encryption at Rest | TLS/In-Transit | Key Management | Property to Check |
|----------------|-------------------|----------------|----------------|-------------------|
| **SQL Server** | TDE | TLS 1.2+ enforced | Platform-managed vs CMK | `minimum_tls_version`, `transparent_data_encryption` |
| **Storage Account** | Infrastructure encryption | HTTPS only | Platform vs CMK | `enable_https_traffic_only`, `infrastructure_encryption_enabled` |
| **Virtual Machine** | Azure Disk Encryption | N/A (disk-level) | Platform vs CMK | `azurerm_disk_encryption_set` |
| **AKS** | Secrets encryption at rest | Node-to-node TLS | Platform vs CMK | `encryption_at_rest_enabled` |
| **Key Vault** | Always encrypted | TLS 1.2+ | HSM-backed vs software | `sku_name` (premium vs standard) |
| **Cosmos DB** | Always encrypted | TLS 1.2+ | Platform vs CMK | `key_vault_key_id` |
| **PostgreSQL** | Always encrypted | TLS enforced | Platform vs CMK | `ssl_enforcement_enabled`, `ssl_minimal_tls_version_enforced` |
| **MySQL** | Always encrypted | TLS enforced | Platform vs CMK | `ssl_enforcement_enabled`, `tls_version` |
| **App Service** | N/A (stateless) | HTTPS only, min TLS | N/A | `https_only`, `minimum_tls_version` |
| **Function App** | N/A (stateless) | HTTPS only, min TLS | N/A | `https_only`, `minimum_tls_version` |
| **API Management** | N/A (gateway) | TLS versions | Managed via policies | `minimum_api_version`, SSL policies |
| **Container Registry** | Always encrypted | TLS 1.2+ | Platform vs CMK | `encryption` block |
| **Redis Cache** | Always encrypted | TLS enforced | Platform vs CMK | `minimum_tls_version`, `enable_non_ssl_port` = false |
| **Event Hub** | Always encrypted | TLS 1.2+ | Platform vs CMK | `minimum_tls_version` |
| **Service Bus** | Always encrypted | TLS 1.2+ | Platform vs CMK | `minimum_tls_version` |

**TLS Version Risk Assessment:**
- TLS 1.0 / 1.1: â›” **CRITICAL** - Deprecated, cryptographically broken (POODLE, BEAST)
- TLS 1.2: âœ… **SECURE** - Current standard
- TLS 1.3: âœ… **BEST** - Latest, faster handshake

**Key Management Tiers:**
1. âœ… Customer-Managed Keys (CMK) in HSM - Compliance requirement (PCI-DSS, HIPAA)
2. âœ… Customer-Managed Keys (CMK) in Key Vault - Customer control
3. ğŸŸ¡ Platform-Managed Keys (PMK) - Microsoft manages, auto-rotation
4. ğŸ”´ No encryption - Unacceptable for ANY data

**Encryption Finding Categories:**
- **ENC-001:** Legacy TLS versions allowed (1.0/1.1)
- **ENC-002:** HTTPS not enforced (HTTP allowed)
- **ENC-003:** Encryption at rest disabled
- **ENC-004:** Platform-managed keys for regulated data (should be CMK)
- **ENC-005:** No key rotation policy configured

### Phase 3: Systematic Five Pillars Security Review

**For EVERY resource type detected in Phase 1/2:**

#### Step 1: Resource Inventory
- [ ] List all resources from Terraform/IaC
- [ ] Group by type (Key Vault, Storage, SQL, VMs, AKS, etc.)
- [ ] Identify data classification per resource (TIER 1-5)

#### Step 2: Network Security (Pillar 1)
For each resource:
- [ ] Check Network Security Matrix above
- [ ] Verify network restrictions configured
- [ ] Flag missing restrictions as findings (NET-xxx)
- [ ] Assess public vs private endpoint usage

#### Step 3: Access Control (Pillar 2)
For each resource:
- [ ] Check Access Control Matrix above
- [ ] Verify modern auth (Managed Identity/AAD/IAM)
- [ ] Flag legacy auth (keys/passwords) as findings (ACC-xxx)
- [ ] Check RBAC/IAM scope (least privilege?)
- [ ] Verify JIT/expiry configured for SPNs/keys

#### Step 4: Audit Logging (Pillar 3)
For each resource:
- [ ] Check Audit Logging Matrix above
- [ ] Verify logging enabled (diagnostic settings/auditing)
- [ ] Check retention period (90+ days for compliance)
- [ ] Flag missing logging as findings (LOG-xxx)

#### Step 5: Log Consumption (Pillar 4)
For entire environment:
- [ ] Detect Log Analytics Workspace / CloudWatch / Cloud Logging
- [ ] Detect SIEM (Sentinel / GuardDuty / Security Command Center)
- [ ] Check alert rules configured
- [ ] Assess monitoring maturity (Level 0-4)
- [ ] Create environment-level findings (ENV-xxx) if infrastructure missing

#### Step 6: Data Protection (Pillar 5)
For each resource:
- [ ] Check Data Protection Matrix above
- [ ] Verify encryption at rest enabled
- [ ] Check key management (CMK vs platform-managed)
- [ ] Verify TLS/HTTPS enforcement
- [ ] Check minimum TLS version (1.2+ required)
- [ ] Flag legacy TLS (1.0/1.1) as CRITICAL findings (ENC-xxx)
- [ ] Match encryption controls to data classification (TIER 1 needs CMK)

#### Step 7: Cross-Reference
- [ ] Compare findings to architecture diagram (no missing services)
- [ ] Validate data classification affects severity scoring
- [ ] Ensure every resource assessed against all 5 pillars
- [ ] Calculate monitoring maturity level (0-4)

**Expected Output:**
- 5-10x more findings than ad-hoc approach
- Systematic coverage (no blind spots)
- Environment-level findings (monitoring infrastructure)
- Encryption-specific findings with TLS version checks
- Data classification context in every finding

---

These matrices enable systematic security assessment. Use them in every Phase 3 security review.
